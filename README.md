# SuZhou_CreditPrediction  
An algorithmic topic of the first Taihu Credit Big Data Innovation Application Contest ! Rank : 5 / 147 !  
 [首届太湖信用大数据创新应用大赛](http://cbdc.youedata.com/)[首届太湖信用大数据创新应用大赛](http://news.xinhua08.com/a/20181228/1791330.shtml)是由苏州市政府主办的比赛，旨在挖掘大数据在城市信用体系建设中的创新应用，深化大数据与信用体系的融合发展，推动信用应用场景创新，为苏州进一步深化信用建设提供专业支撑。我们参加的是其中一道算法题，背景为企业合规风险预测。这个赛题以企业为中心，提供了企业主体在多方面留下的行为足迹信息，要求经过数据分析及模型训练后，能准确预测企业在一段时间内的失信概率和行政处罚概率。  
最终排名：5 / 147  
附：[github地址](https://github.com/RHKeng/SuZhou_CreditPrediction)  
  
* 1 **比赛简介**      
本次比赛是[首届太湖信用大数据创新应用大赛](http://news.xinhua08.com/a/20181228/1791330.shtml)的一个算法子赛题，背景是企业合规风险预测。本赛题以企业为中心，提供企业的基本资料、行政许可、税务登记、资质年检、表彰荣誉、异常记录等多方面的行为数据，预测企业在一段时间内的失信概率和行政处罚概率，评价指标是两个预测概率的auc均值。我们本次比赛的流程主要包括赛题分析，数据分析，数据清洗，特征工程，数据分布抽样，模型训练，模型融合和后处理八个部分。  
  
* 2 **问题分析**    
本次比赛属于实际落地场景中的工业化数据需求，且应用场景的专业性和领域性强。因此在做数据分析和模型设计时，需充分理解和考虑相关行业的背景知识，结合企业信用的实际评估场景，针对给出的数据分析其规律，提取特征进行模型训练。  
通过初步分析数据，我们依据企业ID为关联主键，分别构建失信和行政处罚两个子问题的训练集，分别训练失信和行政处罚的二分类模型，得到某个企业在一段时间内失信和被行政处罚的概率。针对在比赛中出现的正负样本不均衡情况，我们采用了类似EasyEnsemble的负样本采样方法来解决。  
  
* 3 **数据分析与数据清洗**   
（1）训练集共14个表格，其中两个表格是失信和行政处罚名单，用来打标签，其余的训练集和测试集保持一致。通过分析可以发现，除了企业基本信息和机构设立登记信息表格外，其余表格只包括部分的企业，有一些表格只有很小部分的企业信息。比如，在构造训练集过后，共有56731个企业，法人行政许可注销信息中只有700左右的企业，企业表彰荣誉信息只有1000左右个企业，双打办打击侵权假冒处罚案件信息只有400左右个企业，空缺情况严重。在利用这些企业缺失严重的表格时，我们有针对性地做了一些特征。  
![image.png](https://upload-images.jianshu.io/upload_images/12207295-6c46c1e4b932d418.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  
（2）在构造训练集过后，通过分析可以看到，不管是失信数据集还是被行政处罚数据集都存在正负样本不均衡的情况，其中失信样本更为严重，只占了1.5%。  
![image.png](https://upload-images.jianshu.io/upload_images/12207295-b1c8b88bffb68ff8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  
（3）通过分析失信数据的省份分布可以看到，江苏省和非江苏省的比例是46822：9054，非江苏省的数据都不是失信企业，这是本次比赛的一个小trick，应该跟主办方数据收集不全有关。  
（4）通过分析表格字段，发现部分数据字段有较多的空缺值，如企业税务登记信息表中的机构全称英文，备注等，有些字段甚至空缺了90%以上。对空缺值本身没有意义的字段，由于空缺值过多已经不能很好的给模型提供训练信息，因此进行了剔除。  
（5）数据中存在取值单一的数据字段，如企业税务登记信息表中的数据状态，数据来源等，所有企业的数据取值都一样，并不能区分企业之间的特性，因此进行了字段剔除。  
（6）数据中存在重复意义的数据字段，如企业税务登记信息中的信息部门提供编码和信息部门提供名称，两个数据其实表示的都是部门标识信息，只是一个是中文一个是编码。对于计算机的识别来说这两者意义相同，因此只需保留一个。  
（7）某些字段中含有同义词，企业税务登记信息表中法定代表人证件名称字段。该字段中既有“身份证”，又有“居民身份证”，两者其实表达的是同一个意思，但在计算机看来却是不同的两个词，因此需要进行名称统一处理。  
（8）数据中存在一些拼接字段，例如机构设立（变更）登记信息表中的经营范围字段，该字段值均为企业经营种类的罗列拼接，如“销售：空压机配件、节能产品、净化设备、机电设备、模具及配件、冲床及配件、电子产品……”。考虑做文本处理，需要对该信息进行分词处理，并考虑通过自然语言处理技术（NLP）将经营范围转化为计算机更易识别的向量表示。  
  
* 4 **训练样本构造**   
本次比赛主办方共提供了14张表格，通过分析表格字段缺失情况，我们利用企业基础信息和机构设立登记信息表格作为基础表，通过企业id作为关键主键进行拼接，并根据两个标签表格（实行被执行人名单，双公示--法人行政处罚信息）对训练集打标签，完成训练集的构造。由于复赛允许使用初赛数据，而且通过分析可以看到，初赛和复赛数据分布一致，因此我们采用了简单的数据拼接方法。  
考虑到失信问题的正负样本比例太悬殊，影响模型训练的精确度。我们对比了正样本过采样，负样本采样的效果，最终采用了类似EasyEnsemble的负样本采样方法。将负样本随机分成等量的两份，分别与正样本结合构造成两套失信训练数据集。将负样本拆分成多模型训练而不是单纯进行抽样舍弃，使得模型能够充分学习到所有样本的数据信息。  
![image.png](https://upload-images.jianshu.io/upload_images/12207295-5154e7232004d0ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  
附：EasyEnsemble算法是一种有效的不均衡数据分类方法。它将多数类样本随机分成多个子集，每个子集分别与少数类合并，得到多个新的训练子集，并利用每个训练子集训练一个AdaBoost基分类器，最后集成所有基分类器，得到最终的分类器。EasyEnsemble算法有效解决了数据不均衡问题，且减少欠采样造成的多数类样本信息损失。  
  
* 4 **特征工程**  
根据以上数据分析及赛题背景知识的理解，我们从以下几个方面构造数据特征：  
（1）基础特征：属于数据表直接提供的企业基础信息，如企业注册资金、企业住所所在地省份编码、企业行业门类代码等。  
思考：通过观察模型训练出来的特征重要度可以看到，基础特征的重要度通常来说都会比较高，比如企业注册资金和企业行业门类代码，主要是因为基础特征基本没有缺失值，而且涵盖了所有的企业数据，能够很好地区分企业间的特性，给模型训练提供较多的信息，所以基础特征其实很重要。  
（2）表征特征：经过向量化处理或者编码处理的表征特征，如企业机构经营范围的word2vec向量，行政区划代码中的省市县编码等。  
企业机构经营范围的word2vec表征特征：  
![image.png](https://upload-images.jianshu.io/upload_images/12207295-dbd9defb03b0daea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  
分词：我们采用的是目前比较成熟的中文分词工具——jieba分词对该字段值进行分词处理，并剔除了没有意义的标点符号。  
词向量表示：将该字段分词后的所有语句，放入word2vec模型中进行语义训练，得到每一个词语的10维向量表示，且语义相似的词语，其词向量之间的距离更近。  
范围向量表示：对语句中的每一个词的词向量进行加和平均运算，得到该样本的企业经营范围的10维向量表示，并作为10维特征放入模型训练。  
思考：原始的“企业经营范围”字段的取值类似于“销售：空压机配件、节能产品、净化设备、机电设备、模具及配件、冲床及配件、电子产品……”，对于模型来说识别这样一长串的文本并不友好。如果将这些文字描述转化为向量形式的范围描述，则会帮助模型更好的理解该字段的信息。我们一开始只是简单地对企业经营范围字段做了一个encoder然后直接丢到模型里面训练，发现特征重要度特别高，然后考虑进行特征改进，进一步细化这个特征。其实通过word2vec模型得出的词向量，语义相近的词语距离会更近，而词向量每一维的值代表一个具有一定的语义和语法上解释的特征（对应于本特征，其实就是把经营范围提取成10个维度，可能一个维度是代表制造、零售、咨询、租赁、代理等某一方面的权重）。因此对句子中的词向量进行加和平均然后丢到模型里面训练是可以给模型提供一些语义上的信息的。  
企业地址信息表征特征：  
企业机构地址与行政区划这两个字段其实都是表示企业机构所在的位置，只是机构地址是描述性文字，而行政区划是固定的6位数字且各部分有其固定的编码意义。因此我们对行政区划提取了几个特征，包括2位省份编码、2位城市编码、2位区县编码、4位省份+城市编码、6位省市县编码。  
![image.png](https://upload-images.jianshu.io/upload_images/12207295-1308d7f8c47f8153.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  
思考：我们一开始是简单对企业地址做了encoder然后丢到模型里面，发现重要度很高，然后想到行政区划也能一定表示企业地址，因此做了细化。省份编码、城市编码和区县编码特征其实对企业来说是具有一定的区分度的（具体可以从转化率观察出来），因此加入这些特征是可以给模型带来额外的信息的，有助于提高模型精度。  
（3）日期特征：从各项日期字段中提取出的年、月、日特征，以及相关日期字段之间的时间差（如企业核准日期与成立日期之间的差值）。  
思考：这里有一个强特，是企业核准日期和成立日期的差值，应该是跟实际场景比较相关，企业核准日期相对成立日期来说越晚，一定程度上失信和被行政处罚的概率会增加。  
（4）数量特征：企业在各个子表中的出现次数。如企业的税务登记次数、企业被表彰次数、企业进行许可资质年检的次数等。  
思考：由于很多表格只囊括了很小一部分企业，表格中很多字段都不能提供信息，采用计数方法统计企业在表格中出现次数，能把该表格的大部分信息提取出来。  
（5）统计特征：包括数值型特征的数学统计指标，如企业招聘人数的最大值、最小值、均值、方差等。还包括标签型特征的独立取值个数，如企业招聘职业的种类数等。 
根据模型训练过程中统计出的特征重要性进行了特征筛选，剔除掉重要性太低，对模型没什么贡献的特征。最终选取了56个特征用于企业失信概率模型；82个特征用于企业处罚概率模型。  
  
* 5 **模型选择**  
（1）失信模型  
我们针对构造出来的两套失信训练数据集，利用不同模型之间的差异性，分别训练了一个xgboost模型和一个lightgbm模型，根据xgboost和lightgbm接口修改了正负样本权重，并进行了五折交叉平均，考虑到本次比赛的评价指标是auc，auc比较注意样本预测出来的概率相对性，因此我们采用了rank融合的方法进行模型融合。此外，我们也做了结果后处理，考虑到数据集中非江苏省企业的失信数据异常，我们再最终的模型结果中将非江苏省企业的失信概率修改为0，从而更吻合比赛数据集的分布规律。  
（2）被行政处罚模型  
被行政处罚模型没有进行样本采样，在修改了正负样本权重后，分别训练了xgboost模型和lightgbm模型，进行了五折交叉平均和基于rank融合的模型融合。（这里不用进行负样本采样，因为正负样本比例大概为1:5，不是严重失调，可以直接进行五折交叉训练，只需修改一下训练模型的正负样本权重即可）  
  
附：rank融合  
![image.png](https://upload-images.jianshu.io/upload_images/12207295-3b97ae390f3215e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  
适合排序评估指标，比如auc之类的，其中weight_i为该模型权重，权重为1表示平均融合，rank_i表示样本的升序排名 ，也就是越靠前的样本融合后也越靠前，能较快的利用排名融合多个模型之间的差异，而不用去加权样本的概率值融合。  
  
* 6 **赛后总结**    
本次比赛，虽然时间跨度也挺长的，但是其实我们队伍用在这个比赛的时间并不是特别多，不过虽然如此，我们还是做了很多的工作，包括赛题分析，数据分析，数据清洗，特征工程，数据分布抽样，模型训练，模型融合和后处理，最终复赛线上是第三名。现在想起来，其实还是有的地方做得不是特别好，最大的一个问题是我们对赛题背景确实了解得不多（到答辩才知道双打办打击侵权假冒处罚案件信息跟行政处罚的关系，犯罪嫌疑人和确认犯罪的人），很多时候是用一些数据挖掘的手段来做这个比赛，但是我们这次比赛还是有挺多亮点的，如下所示：  
（1）本次比赛的数据是从真实场景中提取出来的，主办方也没有做特别多的脱敏（除了企业名称这些敏感字段），因此很贴合实际场景，其实很有挑战性。工业数据其实是很乱，很杂，很多噪声的，因此我们是有针对性地做了很多数据分析和数据清洗，这给我们的模型提供了一个基础的保障；  
（2）考虑到失信问题的正负样本比例太悬殊，影响模型训练的精确度。我们对比了正样本过采样，负样本欠采样的效果，最终采用了类似EasyEnsemble的负样本采样方法。将负样本随机分成等量的两份，分别与正样本结合构造成两套失信训练数据集。将负样本拆分成多模型训练而不是单纯进行抽样舍弃，使得模型能够充分学习到所有样本的数据信息；  
（3）虽然我们对赛题背景理解不足，但是我们还是提取了很多强特，包括企业核准成立时间差（这个跟赛题背景有关，企业核准日期相对成立日期来说越晚，一定程度上失信和被行政处罚的概率会增加），企业经营范围word2vec特征和行政机构编码特征（从特征细化角度入手）以及基础表格数量特征（考虑到缺失情况），大大提高了模型精度；  
（4）关于模型融合，针对auc评价指标，我们对比了加权融合和rank融合，最终采用了rank融合提高模型精度以及鲁棒性；  
（5）通过数据分析，我们也发现了一些数据分布的trick，比如非江苏省失信数据的异常，并针对异常进行了模型结果的优化处理；  
答辩过后，发现其实复赛2到5名队伍的方案都差不多，各有一些小特色，比较惊艳的是第一名的方案：关于一些文本特征，比如企业经营范围，机构地址，他们做了一个stacking的操作，这个可以提高5个千的成绩。具体做法是，先对所有文本特征进行组合分词，然后统计每个词的tf，选择tf较高的一部分词语作为特征候选集，依据样本是否包含该词语提取01特征，然后用一个lr进行stacking训练（这里用lr是因为样本特征比较稀疏，树模型不太支持稀疏特征，而lr可以很好支持），得到的结果作为一个特征丢到主模型里面训练。  
  
附：稀疏特征为什么不好用gbdt等树模型？  
gbdt这类boosting集成分类器模型的算法，是典型的贪心算法，在当前节点总是选择对当前数据集来说最好的选择，这样高维稀疏数据集里很多“小而美”的数据就被丢弃了，因为它对当前节点来说不是最佳分割方案(比如，关联分析里，支持度很低置信度很高的特征)。但是高维数据集里面，对特定的样本数据是有很强预测能力的，比如你买叶酸，买某些小的孕妇用品品类，对应这些人6个月后买奶粉概率高达40%，但叶酸和孕妇用品销量太小了，用户量全网万分之一都不到，这种特征肯定是被树算法舍弃的（增益不够），哪怕这些特征很多很多，它仍是被冷落的份。因此选择svm和lr这种能提供最佳分割平面的算法可能会更好。
